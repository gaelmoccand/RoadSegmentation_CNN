Data

For this project, a set N = 100 training images was provided with a size of 400x400 pixels in rgb. The set contains different aerial pictures of mostly urban areas with roads.
Together with this training set, the corresponding 100 groundtruth grayscale images is also provided. In this groundtruth set the different roads are represented in white and the 
rest of the image is black. They are few pixels on the border of the road which are grey. So an appropriate threshold must be defined to choose whether theses pixels belongs to 
the road or not. 
However, theses groundtruth images are required to be converted into label images containing only the class label road (y=1). 
This pixel threashold has a direct impact on the width of the road label in the label image. The pixel grayscale threshold which decides if the current pixel belongs to the road label
or not was set to 25% of the pixel grayscale following the python examples which were provided. 

The small size of the training set (N=100 images only) is not big enough to avoid over-fitting of the model. Moreover in order to train any convolutional neural network correctly
it is therefore necessary to augment the data set using some transformations. Analyzing the training set, it is obvious that the set contains many pictures
of vertical and horizontal roads. Thanks to this information, it has been decided to use rotation of every 5 degrees for each images (5,10,15,20 degrees so on) to generate a set of images 
for every directions of the roads and avoid over-fitting on certain directions.
Therefore for each image of the original training set 71 images will be generated using the rotations, resulting of a new training dataset of N=7100 images. 
This augmented training set is then more suitable for training different convolutional neural networks. 

The features extraction methodology described in the provided examples (segment_aerial_images.ipynb and tf_aerial_images.py) using a patch of size 16x16 pixels has a drawback. 
This methodology based on patches flatten the data and therefore throws away information about the 2D structure of the image. The main advantage of this technique is the fact that 
it requires less computer power than a fully pixel based methodology. 

Concerning the cross-validation, the training set has been split into 2 parts 80% for the training (5680 images) and 20% (1420 images) for the testing all methodologies. 
In the case of the convolutional neural network approach a batch of 50 images is used for each training step with a reduced size of the input image (224x224 pixels) 
in order to speed up the training of the neural network.